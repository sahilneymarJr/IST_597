# -*- coding: utf-8 -*-
"""Copy of IST597_Building_CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mXKQDM3Gd0juAYfI7UZlHO3lok0ruJ2s

# In this tutorial we will build basic CNN for image classification.
Author :- Ankur Mali
* We will define our model and learn how to use keras module to build custom layers
* We will also design our own training loop, that is identical to model.fit in Keras.
* The aim of this excercise is to teach, how to use exisiting Tensorflow API to construct our own module and integrate it with tf.keras API.
"""

import tensorflow as tf
import numpy as np
import sys
import matplotlib.pyplot as plt

np.random.seed(5364*3)

"""#Things to do
* Remember to Normalize your data and create validation split from train set.
* Learn about tf.data, tf.slices and also tf.records
"""

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
x_val = x_train[50000:60000]
x_train = x_train[0:50000]
y_val = y_train[50000:60000]
y_train = y_train[0:50000]
x_train = x_train.astype(np.float32).reshape(-1,28,28,1) / 255.0
x_val = x_val.astype(np.float32).reshape(-1,28,28,1) / 255.0
x_test = x_test.astype(np.float32).reshape(-1,28,28,1) / 255.0
y_train = tf.one_hot(y_train, depth=10)
y_val = tf.one_hot(y_val, depth=10)
y_test = tf.one_hot(y_test, depth=10)
print(x_train.shape)
print(x_test.shape)
print(x_val.shape)
train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
train_dataset = train_dataset.shuffle(buffer_size=1024).batch(128, drop_remainder=True)
train_dataset_full = train_dataset.shuffle(buffer_size=1024).batch(len(train_dataset))
val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))
val_dataset = val_dataset.batch(128, drop_remainder=True)
test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))
test_dataset = test_dataset.batch(128, drop_remainder=True)
print(len(train_dataset))
print(len(test_dataset))

class ImageRecognitionCNN(tf.keras.Model):
    
    def __init__(self, num_classes, device='cpu:0', checkpoint_directory=None):
        ''' Define the parameterized layers used during forward-pass, the device
            where you would like to run the computation (GPU, TPU, CPU) on and the checkpoint
            directory.
            
            Args:
                num_classes: the number of labels in the network.
                device: string, 'cpu:n' or 'gpu:n' (n can vary). Default, 'cpu:0'.
                checkpoint_directory: the directory where you would like to save or 
                                      restore a model.
        ''' 
        super(ImageRecognitionCNN, self).__init__()
        
        # Initialize layers
        self.conv1 = tf.keras.layers.Conv2D(64, 3, padding='same', activation=None)
        # self.bn_1 = batch_norm(64)
        self.conv2 = tf.keras.layers.Conv2D(64, 3, padding='same', activation=None)
        # self.bn_2 = batch_norm(64)
        self.pool1 = tf.keras.layers.MaxPool2D()
        self.conv3 = tf.keras.layers.Conv2D(64, 3, padding='same', activation=None)
        # self.bn_3 = batch_norm(64)
        self.conv4 = tf.keras.layers.Conv2D(64, 3, padding='same', activation=None)
        # self.bn_4 = batch_norm(64)
        # self.pool2 = tf.keras.layers.MaxPool2D()
        # self.conv5 = tf.keras.layers.Conv2D(64, 3, padding='same', activation=None)
        # self.pool2 = tf.keras.layers.MaxPool2D()
        # self.conv6 = tf.keras.layers.Conv2D(64, 3, 2, padding='same', activation=None)
        # self.conv7 = tf.keras.layers.Conv2D(64, 1, padding='same', activation=None)
        self.conv8 = tf.keras.layers.Conv2D(num_classes, 1, padding='same', activation=None)
        # self.bn_5 = batch_norm(num_classes)
        
        # Define the device 
        self.device = device
        
        # Define the checkpoint directory
        self.checkpoint_directory = checkpoint_directory
        self.acc = tf.keras.metrics.Accuracy()
        
        self.beta_1 = tf.Variable(tf.random.normal([1, 28, 28, 64]), trainable = True)
        self.gamma_1 = tf.Variable(tf.random.normal([1, 28, 28, 64]), trainable = True)
        self.global_mean_1 = tf.Variable(tf.zeros([1, 28, 28, 64]), trainable = False)
        self.global_var_1 = tf.Variable(tf.zeros([1, 28, 28, 64]), trainable = False)
      
        self.beta_2 = tf.Variable(tf.random.normal([1, 14, 14, 64]), trainable = True)
        self.gamma_2 = tf.Variable(tf.random.normal([1, 14, 14, 64]), trainable = True)
        self.global_mean_2 = tf.Variable(tf.zeros([1, 14, 14, 64]), trainable = False)
        self.global_var_2 = tf.Variable(tf.zeros([1, 14, 14, 64]), trainable = False)
      
        self.beta_3 = tf.Variable(tf.random.normal([1, 7, 7, 64]), trainable = True)
        self.gamma_3 = tf.Variable(tf.random.normal([1, 7, 7, 64]), trainable = True)
        self.global_mean_3 = tf.Variable(tf.zeros([1, 7, 7, 64]), trainable = False)
        self.global_var_3 = tf.Variable(tf.zeros([1, 7, 7, 64]), trainable = False)
      
        self.beta_4 = tf.Variable(tf.random.normal([1, 3, 3, 64]), trainable = True)
        self.gamma_4 = tf.Variable(tf.random.normal([1, 3, 3, 64]), trainable = True)
        self.global_mean_4= tf.Variable(tf.zeros([1, 3, 3, 64]), trainable = False)
        self.global_var_4 = tf.Variable(tf.zeros([1, 3, 3, 64]), trainable = False)

        self.beta_5 = tf.Variable(tf.random.normal([1, 1, 1, 10]), trainable = True)
        self.gamma_5 = tf.Variable(tf.random.normal([1, 1, 1, 10]), trainable = True)
        self.global_mean_5 = tf.Variable(tf.zeros([1, 1, 1, 10]), trainable = False)
        self.global_var_5 = tf.Variable(tf.zeros([1, 1, 1, 10]), trainable = False)

    def BN(self, X, beta, gamma, global_mean, global_var, train, decay=0.9, epsilon=1e-5):

        if train:
            batch_mean = tf.reduce_mean(X, axis=(0,1,2))
            batch_var = tf.math.reduce_variance(X, axis=(0,1,2))
            X = tf.math.divide((X - batch_mean), tf.math.sqrt(batch_var + epsilon))
            
            X = gamma*X + beta
            mean = global_mean*decay + batch_mean*(1 - decay)
            global_mean.assign(mean)

            var = global_var*decay + batch_var*(1 - decay)
            global_var.assign(var)
          
        else:
            
            X = tf.math.divide((X - global_mean), tf.math.sqrt(global_var + epsilon))
            X = gamma*X + beta
          
        return X

    def predict(self, images, training):
        """ Predicts the probability of each class, based on the input sample.
            
            Args:
                images: 4D tensor. Either an image or a batch of images.
                training: Boolean. Either the network is predicting in
                          training mode or not.
        """
    
        x = self.conv1(images)
        # print(x.shape)
        x = self.BN(x, self.beta_1, self.gamma_1, self.global_mean_1, self.global_var_1, training)
        x = tf.nn.relu(x)
        x = self.pool1(x)
        x = self.conv2(x)
        # print(x.shape)
        x = self.BN(x, self.beta_2, self.gamma_2, self.global_mean_2, self.global_var_2, training)
        x = tf.nn.relu(x)
        x = self.pool1(x)
        x = self.conv3(x)
        # print(x.shape)
        x = self.BN(x, self.beta_3, self.gamma_3, self.global_mean_3, self.global_var_3, training)
        x = tf.nn.relu(x)
        x = self.pool1(x)
        x = self.conv4(x)
        # print(x.shape)
        x = self.BN(x, self.beta_4, self.gamma_4, self.global_mean_4, self.global_var_4, training)
        x = tf.nn.relu(x)
        x = self.pool1(x)
        x = self.conv8(x)
        # print(x.shape)
        x = self.BN(x, self.beta_5, self.gamma_5, self.global_mean_5, self.global_var_5, training)
        #x = tf.nn.relu(x)
        #print(x.shape)
        x = tf.reshape(x, (-1, 1, 10))
        #x = tf.keras.layers.Flatten(x)
        return x


    # def BN(self, )
    def loss_fn(self, images, target, training):
        """ Defines the loss function used during 
            training.         
        """
        preds = self.predict(images, training)
        #print(preds.shape)
        #print(target.shape)
        loss = tf.nn.softmax_cross_entropy_with_logits(labels=target, logits=preds)
        return loss


    def grads_fn(self, images, target, training):
        """ Dynamically computes the gradients of the loss value
            with respect to the parameters of the model, in each
            forward pass.
        """
        with tf.GradientTape() as tape:
            loss = self.loss_fn(images, target, training)
        return tape.gradient(loss, self.variables)
    
    def restore_model(self):
        """ Function to restore trained model.
        """
        with tf.device(self.device):
            # Run the model once to initialize variables
            dummy_input = tf.constant(tf.zeros((1,48,48,1)))
            dummy_pred = self.predict(dummy_input, training=False)
            # Restore the variables of the model
            saver = tf.Saver(self.variables)
            saver.restore(tf.train.latest_checkpoint
                          (self.checkpoint_directory))
    
    def save_model(self, global_step=0):
        """ Function to save trained model.
        """
        tf.Saver(self.variables).save(self.checkpoint_directory, 
                                       global_step=global_step)   
    
    # def compute_accuracy(self, input_data):
    #     """ Compute the accuracy on the input data.
    #     """
    #     with tf.device(self.device):
    #         #acc = tf.metrics.Accuracy()
    #         for step ,(images, targets) in enumerate(input_data):
    #             # Predict the probability of each class
    #             #print(targets.shape)
    #             logits = self.predict(images, training=False)
    #             # Select the class with the highest probability
    #             #print(logits.shape)
    #             logits = tf.nn.softmax(logits)
    #             logits = tf.reshape(logits, [-1, 10])
    #             targets = tf.reshape(targets, [-1,10])
    #             preds = tf.argmax(logits, axis=1)
                
    #             #m1.update_state
    #             # Compute the accuracy
    #             #print(preds.shape)
    #             acc(tf.reshape(targets, preds))
    #     return acc

    def compute_accuracy_2(self, images, targets):
        """ Compute the accuracy on the input data.
        """
        with tf.device(self.device):
            
            # Predict the probability of each class
            logits = self.predict(images, training=False)
            # Select the class with the highest probability
            
            logits = tf.nn.softmax(logits)
            logits = tf.reshape(logits, [-1, 10])
            targets = tf.reshape(targets, [-1,10])
            preds = tf.argmax(logits, axis=1)
            goal = tf.argmax(targets, axis=1)
            self.acc.update_state(goal, preds)
            # Compute the accuracy
            result = self.acc.result().numpy()
        return result

  
    def fit_fc(self, training_data, eval_data, test_data, optimizer, num_epochs=500, 
            early_stopping_rounds=10, verbose=10, train_from_scratch=False):
        """ Function to train the model, using the selected optimizer and
            for the desired number of epochs. You can either train from scratch
            or load the latest model trained. Early stopping is used in order to
            mitigate the risk of overfitting the network.
            
            Args:
                training_data: the data you would like to train the model on.
                                Must be in the tf.data.Dataset format.
                eval_data: the data you would like to evaluate the model on.
                            Must be in the tf.data.Dataset format.
                optimizer: the optimizer used during training.
                num_epochs: the maximum number of iterations you would like to 
                            train the model.
                early_stopping_rounds: stop training if the loss on the eval 
                                       dataset does not decrease after n epochs.
                verbose: int. Specify how often to print the loss value of the network.
                train_from_scratch: boolean. Whether to initialize variables of the
                                    the last trained model or initialize them
                                    randomly.
        """ 
    
        if train_from_scratch==False:
            self.restore_model()
        
        # Initialize best loss. This variable will store the lowest loss on the
        # eval dataset.
        best_loss = 999
        
        # Initialize classes to update the mean loss of train and eval
        train_loss = tf.keras.metrics.Mean('train_loss')
        eval_loss = tf.keras.metrics.Mean('eval_loss')
        test_loss = tf.keras.metrics.Mean('test_loss')
        acc_train = tf.keras.metrics.Mean('train_acc')
        acc_val = tf.keras.metrics.Mean('val_acc')
        acc_test = tf.keras.metrics.Mean('test_acc')
        
        # Initialize dictionary to store the loss history
        self.history = {}
        self.history['train_loss'] = []
        self.history['eval_loss'] = []
        self.history['train_acc'] = []
        self.history['val_acc'] = []
        self.history['test_acc'] = []
        self.history['test_loss'] = []
        
        # Begin training
        with tf.device(self.device):
            for i in range(num_epochs):
                # Training with gradient descent
                #training_data_x = training_data.shuffle(buffer_size=1024).batch(128)
                for step, (images, target) in enumerate(training_data):
                    grads = self.grads_fn(images, target, True)
                    optimizer.apply_gradients(zip(grads, self.variables))
                    
                # Compute the loss on the training data after one epoch
                for step, (images, target) in enumerate(training_data):
                    loss = self.loss_fn(images, target, False)
                    accuracy = self.compute_accuracy_2(images,target)
                    acc_train(accuracy)
                    train_loss(loss)
                self.history['train_loss'].append(train_loss.result().numpy())
                self.history['train_acc'].append(acc_train.result().numpy())
                # Reset metrics
                train_loss.reset_states()
                acc_train.reset_states()
                
                # Compute the loss on the eval data after one epoch
                for step, (images, target) in enumerate(eval_data):
                    loss = self.loss_fn(images, target, False)
                    accuracy = self.compute_accuracy_2(images,target)
                    acc_val(accuracy)
                    eval_loss(loss)
                self.history['eval_loss'].append(eval_loss.result().numpy())
                self.history['val_acc'].append(acc_val.result().numpy())
                # Reset metrics
                eval_loss.reset_states()
                acc_val.reset_states()

                for step, (images, target) in enumerate(test_data):
                    loss = self.loss_fn(images, target, False)
                    accuracy = self.compute_accuracy_2(images,target)
                    acc_test(accuracy)
                    test_loss(loss)
                self.history['test_loss'].append(test_loss.result().numpy())
                self.history['test_acc'].append(acc_test.result().numpy())
                # Reset metrics
                test_loss.reset_states()
                acc_test.reset_states()
                
                # Print train and eval losses
                if (i==0) | ((i+1)%verbose==0):
                    print('Train loss at epoch %d: ' %(i+1), self.history['train_loss'][-1])
                    print('Train Acc at epoch %d: ' %(i+1), self.history['train_acc'][-1])
                    
                    print('Eval loss at epoch %d: ' %(i+1), self.history['eval_loss'][-1])
                    print('Eval Acc at epoch %d: ' %(i+1), self.history['val_acc'][-1])

                    print('Test loss at epoch %d: ' %(i+1), self.history['test_loss'][-1])
                    print('Test Acc at epoch %d: ' %(i+1), self.history['test_acc'][-1])

                # Check for early stopping
                if self.history['eval_loss'][-1]<best_loss:
                    best_loss = self.history['eval_loss'][-1]
                    count = early_stopping_rounds
                else:
                    count -= 1
                if count==0:
                    break

# Specify the path where you want to save/restore the trained variables.
checkpoint_directory = 'models_checkpoints/mnist/'

# Use the GPU if available.
device = 'gpu:0'

# Define optimizer.
optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=1e-4)

# Instantiate model. This doesn't initialize the variables yet.
model = ImageRecognitionCNN(num_classes=10, device=device, 
                              checkpoint_directory=checkpoint_directory)

#model = ImageRecognitionCNN(num_classes=7, device=device)

# Train model
model.fit_fc(train_dataset, val_dataset, test_dataset, optimizer, num_epochs=10, 
          early_stopping_rounds=2, verbose=2, train_from_scratch=True)

